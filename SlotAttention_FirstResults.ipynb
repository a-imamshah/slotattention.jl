{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CUDA\n",
    "using Knet\n",
    "using CUDA: CUDA, CuArray\n",
    "using Images\n",
    "using ImageMagick\n",
    "using Random\n",
    "import Base: length, size, iterate, eltype, IteratorSize, IteratorEltype, haslength, @propagate_inbounds, repeat, rand, tail\n",
    "import .Iterators: cycle, Cycle, take\n",
    "using IterTools: ncycle, takenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(123);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64;\n",
    "resolution = 16; #16 for Baseline\n",
    "decoder_res = 2\n",
    "slot_size = 32;\n",
    "num_slots = 4;\n",
    "num_iterations = 3;\n",
    "hidden_dim = 32;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterate (generic function with 435 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_type=(CUDA.functional() ? KnetArray{Float32} : Array{Float32})\n",
    "atype() = array_type\n",
    "atype(x) = convert(atype(),x)\n",
    "\n",
    "function load_image(filename)\n",
    "    img = load(filename)\n",
    "    img = imresize(img, (resolution,resolution))\n",
    "    img = Float64.(channelview(img))\n",
    "    img = img[1:3,:,:]\n",
    "    img = permutedims(img, [2, 3, 1])\n",
    "    img = img .* 2 .- 1\n",
    "end\n",
    "\n",
    "struct CLEVR\n",
    "    images\n",
    "    batchsize::Int\n",
    "    num_instances::Int\n",
    "    shuffle::Bool\n",
    "    function CLEVR(datasetPaths; batchsize::Int=32, shuffle::Bool=false)\n",
    "        nFullBatches, rem = divrem(size(datasetPaths)[end], batchsize)\n",
    "        new(datasetPaths[1:nFullBatches*batchsize], batchsize, nFullBatches*batchsize, shuffle)\n",
    "    end\n",
    "end\n",
    "\n",
    "function length(d::CLEVR)\n",
    "    nFullBatches, rem = divrem(d.num_instances, d.batchsize)\n",
    "    nFullBatches + (rem > 0)*1\n",
    "end\n",
    "\n",
    "function iterate(d::CLEVR, state=ifelse(d.shuffle, randperm(d.num_instances), collect(1:d.num_instances)))\n",
    "    if length(state) > 0\n",
    "        imgBatch = load_image.(d.images[state[1:(length(state) < d.batchsize ? end : d.batchsize)]])\n",
    "        batch = cat(imgBatch..., dims = 4)\n",
    "        state  = state[d.batchsize+1:end]\n",
    "        return atype(batch), state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"//userfiles//ashah20//datasets//tetrominoes//train//images\"\n",
    "files = readdir(trainDataPath);\n",
    "filenames = files[endswith.(files, \".jpg\")]\n",
    "filenames = [\"$(trainDataPath)//$(file)\" for file in filenames];\n",
    "\n",
    "clevrDataset = CLEVR(filenames, batchsize=bs, shuffle = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPath = \"//userfiles//ashah20//datasets//tetrominoes//train//images\"\n",
    "tstfiles = readdir(testDataPath);\n",
    "tstfilenames = tstfiles[endswith.(tstfiles, \".jpg\")]\n",
    "tstfilenames = [\"$(trainDataPath)//$(file)\" for file in tstfilenames];\n",
    "\n",
    "clevrDatasetTest = CLEVR(tstfilenames, batchsize=bs, shuffle = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mse(predictions, targets)\n",
    "    N = length(targets)\n",
    "    y = 1/(2*N) * sum((predictions .- targets).^2)\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Dense\n",
    "    w\n",
    "    b\n",
    "    f\n",
    "    p\n",
    "    bias\n",
    "end\n",
    "\n",
    "function Dense(i::Int,o::Int,f=relu;pdrop=0, bias=true)\n",
    "    w = param(o,i)\n",
    "    if(bias)\n",
    "        b = param0(o)\n",
    "    else\n",
    "        b = 0\n",
    "    end\n",
    "    Dense(w, b, f, pdrop, bias)\n",
    "end\n",
    "\n",
    "function (d::Dense)(x)\n",
    "    if(d.bias)\n",
    "        d.f.(d.w * mat(dropout(x,d.p)) .+ d.b)\n",
    "    else\n",
    "        d.f.(d.w * mat(dropout(x,d.p)))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv; w; b; pad; f; p; end\n",
    "(c::Conv)(x) = c.f.(conv4(c.w, dropout(x,c.p), padding=(c.pad,c.pad), stride=1) .+ c.b)\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,pad::Int, f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), pad, f, pdrop)\n",
    "\n",
    "struct DeConv; w; b; f; p; pad; end\n",
    "(c::DeConv)(x) = c.f.(deconv4(c.w, dropout(x,c.p), padding=(c.pad,c.pad), stride=1) .+ c.b)\n",
    "DeConv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0, pad=1) = DeConv(param(w1,w2,cy,cx), param0(1,1,cy,1), f, pdrop, pad)\n",
    "\n",
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = mse(c(x),y)\n",
    "(c::Chain)(d::CLEVR) = mean(c(x,x) for x in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printdims (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function decoder_in_reshape(x)\n",
    "    batchsize = size(x)[end]\n",
    "    a = reshape(x, (1,1,slot_size, num_slots*batchsize))\n",
    "    b = array_type(ones(decoder_res, decoder_res, slot_size, batchsize*num_slots))\n",
    "    z = b .* a\n",
    "   return z\n",
    "end\n",
    "\n",
    "function flatten(x)\n",
    "    batchsize = size(x)[end]\n",
    "    return array_type(permutedims(reshape(x,(size(x)[1]*size(x)[2],hidden_dim,batchsize)),(2,1,3)))\n",
    "end\n",
    "\n",
    "function printdims(x)\n",
    "    println(size(x))\n",
    "   return x\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now the slot attention module will get this encoded features matrix####\n",
    "### input dimensions of this feature matrix is channel, hxw, batchsize\n",
    "\n",
    "function make3d_2d(x)\n",
    "   return (reshape(x,(size(x)[1],size(x)[2]*size(x)[3])))\n",
    "end\n",
    "\n",
    "function inv_make3d_2d(x, shape)\n",
    "   return (reshape(x,(shape[1],shape[2],shape[3])))\n",
    "end\n",
    "\n",
    "\n",
    "struct SlotAttention\n",
    "    num_slots\n",
    "    slot_size\n",
    "    project_k\n",
    "    project_v\n",
    "    project_q\n",
    "    gru\n",
    "end\n",
    "\n",
    "\n",
    "function SlotAttention(num_slots, slot_size)\n",
    "    project_k = Dense(slot_size, slot_size, bias=false)\n",
    "    project_v = Dense(slot_size, slot_size, bias=false)\n",
    "    project_q = Dense(slot_size, slot_size, bias=false)\n",
    "\n",
    "    gru = RNN(slot_size, slot_size, rnnType=:gru)\n",
    "    return SlotAttention(num_slots, slot_size, project_k, project_q, project_v, gru)\n",
    "end\n",
    "\n",
    "\n",
    "attn_norm_factor = slot_size ^ -0.5\n",
    "epsilon=1e-8\n",
    "\n",
    "\n",
    "function (s::SlotAttention)(x)\n",
    "    batchsize = size(x)[end]\n",
    "    encoded = permutedims(reshape(x,(size(x)[1]*size(x)[2],hidden_dim,batchsize)),(2,1,3))\n",
    "#     println(size(encoded))\n",
    "    flattened_enc = make3d_2d(encoded)\n",
    "#     println(size(flattened_enc))\n",
    "\n",
    "    k = s.project_k(flattened_enc)\n",
    "    k = inv_make3d_2d(k, size(encoded)) #shape = [slot_size, hxw, bs]\n",
    "\n",
    "    v = s.project_v(flattened_enc)\n",
    "    v = inv_make3d_2d(v, size(encoded)) #shape = [slot_size, hxw, bs]\n",
    "    \n",
    "    slots = randn(s.slot_size, s.num_slots, batchsize)\n",
    "    slots = array_type(slots)\n",
    "\n",
    "    for i in 1:num_iterations\n",
    "        prev_slots = slots\n",
    "        slots = make3d_2d(prev_slots)\n",
    "\n",
    "        q = s.project_q(slots)\n",
    "        q = inv_make3d_2d(q, (s.slot_size, s.num_slots, batchsize)) #shape = [slot_size, num_slots, bs]\n",
    "\n",
    "        # batch matrix multiplication\n",
    "        attn_logits = bmm(permutedims(q,(2, 1, 3)),(attn_norm_factor .* k))\n",
    "        # softmax function\n",
    "#         attn = exp.(attn_logits) ./ sum(exp.(attn_logits), dims=1) \n",
    "        attn = softmax(attn_logits, dims=1)\n",
    "\n",
    "        attn = attn .+ epsilon\n",
    "        attn = attn ./ sum(attn, dims=2) #shape = [num_slots, hxw, bs]\n",
    "\n",
    "        updates = bmm(v, permutedims(attn,(2, 1, 3))) #shape = [slot_size, num_slots, bs]\n",
    "        updates = make3d_2d(updates)\n",
    "        prev_slots = make3d_2d(prev_slots)\n",
    "        s.gru.h = prev_slots\n",
    "        slots = s.gru(updates) #shape = [slot_size, num_slots*bs]\n",
    "#         slots = prev_slots\n",
    "        slots = inv_make3d_2d(slots, (s.slot_size, s.num_slots, batchsize)) #shape = [slot_size, num_slots, bs]\n",
    "\n",
    "    end\n",
    "    \n",
    "    return slots\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postprocess_recon (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function postprocess_recon(decoder_out)\n",
    "    decoder_out = reshape(decoder_out, (resolution, resolution, 4, num_slots, bs))\n",
    "\n",
    "    recons = decoder_out[:,:,1:3,:,:]\n",
    "    masks = decoder_out[:,:,4:4,:,:]\n",
    "    masks = softmax(masks, dims=4)\n",
    "\n",
    "    final_recons = reshape(sum(recons .* masks, dims=4), (resolution, resolution, 3, bs))\n",
    "    return final_recons\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((Chain((Conv(P(KnetArray{Float32,4}(5,5,3,32)), P(KnetArray{Float32,4}(1,1,32,1)), 2, Knet.Ops20.relu, 0), Conv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), 2, Knet.Ops20.relu, 0), Conv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), 2, Knet.Ops20.relu, 0), Conv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), 2, Knet.Ops20.relu, 0), Conv(P(KnetArray{Float32,4}(1,1,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), 0, Knet.Ops20.relu, 0), Conv(P(KnetArray{Float32,4}(1,1,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), 0, Knet.Ops20.relu, 0))), SlotAttention(4, 32, Dense(P(KnetArray{Float32,2}(32,32)), 0, Knet.Ops20.relu, 0, false), Dense(P(KnetArray{Float32,2}(32,32)), 0, Knet.Ops20.relu, 0, false), Dense(P(KnetArray{Float32,2}(32,32)), 0, Knet.Ops20.relu, 0, false), GRU(input=32,hidden=32)), decoder_in_reshape, Chain((DeConv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), Knet.Ops20.relu, 0, 0), DeConv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), Knet.Ops20.relu, 0, 0), DeConv(P(KnetArray{Float32,4}(5,5,32,32)), P(KnetArray{Float32,4}(1,1,32,1)), Knet.Ops20.relu, 0, 0), DeConv(P(KnetArray{Float32,4}(5,5,4,32)), P(KnetArray{Float32,4}(1,1,4,1)), Knet.Ops20.relu, 0, 1))), postprocess_recon))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 32; #64 for CLEVR\n",
    "\n",
    "# encoder = Chain(Dense(16*16*3, 512),\n",
    "#                 Dense(512, 1024),\n",
    "#                 Dense(1024, 1024),\n",
    "#                 Dense(1024, slot_size*num_slots)) \n",
    "\n",
    "encoder = Chain(Conv(5,5,3,32,2),\n",
    "                Conv(5,5,32,32,2),\n",
    "                Conv(5,5,32,32,2),\n",
    "                Conv(5,5,32,32,2),\n",
    "                Conv(1,1,32,32,0),\n",
    "                Conv(1,1,32,slot_size,0))\n",
    "#                 flatten)\n",
    "\n",
    "decoder =  Chain(DeConv(5,5,slot_size,hidden_dim, pad=0),\n",
    "                DeConv(5,5,hidden_dim,hidden_dim, pad=0),\n",
    "                DeConv(5,5,hidden_dim,hidden_dim, pad=0),\n",
    "                DeConv(5,5,hidden_dim,4))\n",
    "\n",
    "SlotAttentionModel = Chain(encoder,\n",
    "                        SlotAttention(num_slots, slot_size),\n",
    "                        decoder_in_reshape,\n",
    "                        decoder,\n",
    "                        postprocess_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = first(clevrDataset)\n",
    "x = array_type(x)\n",
    "# x_in = reshape(x, (resolution*resolution*3, bs))\n",
    "# println(size(x))\n",
    "\n",
    "final_recons = SlotAttentionModel(x);\n",
    "# println(size(final_recons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40606538454691565"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = mse(final_recons, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, input_batch)\n",
    "    recon = model(input_batch)\n",
    "    loss = mse(recon, input_batch)\n",
    "#     println(loss)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24-element Array{Param,1}:\n",
       " P(KnetArray{Float32,4}(5,5,3,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(1,1,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(1,1,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,2}(32,32))\n",
       " P(KnetArray{Float32,2}(32,32))\n",
       " P(KnetArray{Float32,2}(32,32))\n",
       " P(KnetArray{Float32,3}(1,1,6336))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,4,32))\n",
       " P(KnetArray{Float32,4}(1,1,4,1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(SlotAttentionModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro gcheck1(ex); esc(:(@gcheck $ex (delta=0.000001, nsample=2, rtol=0.05, atol=0.001, verbose=2))); end\n",
    "\n",
    "function newloss(model, input_batch)\n",
    "    recon = model(input_batch)\n",
    "    loss = sum(recon)\n",
    "#     println(loss)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "checklayer = Conv(5,5,3,32,2)\n",
    "checklayer(first(clevrDataset))\n",
    "\n",
    "@gcheck1 newloss(checklayer, first(clevrDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newloss (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newloss(model, input_batch)\n",
    "    recon = model(input_batch)\n",
    "    loss = sum(recon)\n",
    "#     println(loss)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(P(KnetArray{Float32,4}(5,5,3,32)), P(KnetArray{Float32,4}(1,1,32,1)), 2, Knet.Ops20.relu, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklayer = Conv(5,5,3,32,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16×16×32×64 KnetArray{Float32,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0413504   0.0491704  0.0        …  0.0        0.0         0.0\n",
       " 0.161931    0.140497   0.0271132     0.0163548  0.0232585   0.0\n",
       " 0.107284    0.157575   0.0370177     0.0301597  0.112798    0.193763\n",
       " 0.00547283  0.0465133  0.0           0.0443859  0.11489     0.188296\n",
       " 0.13039     0.0479611  0.0           0.0345231  0.090308    0.213622\n",
       " 0.167704    0.107933   0.0        …  0.0        0.0922607   0.289146\n",
       " 0.1954      0.108772   0.0           0.0        0.0902327   0.326428\n",
       " 0.271932    0.463686   0.0528522     0.0        0.200343    0.420606\n",
       " 0.200699    0.351218   0.0           0.0        0.119166    0.357853\n",
       " 0.275833    0.451271   0.0           0.0        0.0         0.374466\n",
       " 0.225967    0.326942   0.0        …  0.0        0.0         0.302694\n",
       " 0.247283    0.264885   0.0           0.0        0.0         0.318975\n",
       " 0.3326      0.461421   0.0579114     0.0        0.0         0.304236\n",
       " 0.247598    0.393761   0.104503      0.0        0.0         0.255454\n",
       " 0.0745925   0.385006   0.0247541     0.0        0.0429585   0.26134\n",
       " 0.0111002   0.31576    0.0821449  …  0.0        0.00582806  0.255113\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " 0.0        0.0        0.000649435  …  0.000534748  0.257061   0.189314\n",
       " 0.0        0.0        0.0             0.0          0.123989   0.16484\n",
       " 0.0        0.0        0.0460285       0.0177818    0.338978   0.33526\n",
       " 0.089407   0.215456   0.114409        0.0740448    0.355278   0.355554\n",
       " 0.149837   0.278154   0.318813        0.0178849    0.34641    0.365389\n",
       " 0.286015   0.478305   0.326415     …  0.0          0.27418    0.359521\n",
       " 0.352757   0.642085   0.59423         0.0          0.351746   0.345614\n",
       " 0.308767   0.403705   0.177141        0.0          0.215685   0.269277\n",
       " 0.338738   0.500147   0.297746        0.0          0.241267   0.379403\n",
       " 0.249777   0.294581   0.201628        0.0          0.145147   0.28716\n",
       " 0.300101   0.37844    0.212967     …  0.0          0.178278   0.372566\n",
       " 0.291404   0.40189    0.354509        0.0          0.179781   0.330977\n",
       " 0.186832   0.162763   0.110176        0.0          0.120719   0.358565\n",
       " 0.148819   0.173649   0.0694441       0.0          0.230717   0.388987\n",
       " 0.0        0.0605021  0.0             0.0          0.0792667  0.154205\n",
       " 0.0468269  0.0458254  0.0          …  0.0          0.125326   0.316265\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " 0.2488     0.36338   0.183229   …  0.179282   0.0        0.046667\n",
       " 0.216129   0.364296  0.0936816     0.113595   0.0        0.0\n",
       " 0.276118   0.48161   0.212846      0.238448   0.0        0.0229019\n",
       " 0.212916   0.45366   0.111116      0.216075   0.0        0.0176429\n",
       " 0.128787   0.391481  0.0           0.258016   0.0226138  0.0496991\n",
       " 0.0846817  0.341133  0.0265723  …  0.308514   0.118417   0.172247\n",
       " 0.0407104  0.35594   0.0893503     0.191263   0.102669   0.25382\n",
       " 0.117615   0.293513  0.054399      0.182438   0.0694128  0.194864\n",
       " 0.119919   0.373969  0.0453987     0.0741306  0.0333431  0.226055\n",
       " 0.143237   0.333637  0.0141635     0.0        0.0        0.0746201\n",
       " 0.112218   0.391443  0.0218051  …  0.0        0.0        0.150693\n",
       " 0.0967346  0.350277  0.0293712     0.0        0.0        0.0808654\n",
       " 0.158297   0.374179  0.102191      0.0        0.0        0.0662265\n",
       " 0.210529   0.452062  0.258753      0.0        0.0        0.0943502\n",
       " 0.190409   0.375149  0.158426      0.0        0.0        0.0\n",
       " 0.0108786  0.154762  0.0376342  …  0.0        0.0        0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 1] =\n",
       " 0.0        0.0  0.0        0.0        …  0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0        0.0           0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0        0.0           0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0        0.0           0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0        0.0519709     0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0        0.0430852  …  0.0  0.0        0.0        0.0\n",
       " 0.0        0.0  0.0366763  0.290209      0.0  0.0768504  0.0        0.0\n",
       " 0.0        0.0  0.0        0.165285      0.0  0.195592   0.0        0.0\n",
       " 0.0        0.0  0.0        0.134891      0.0  0.364234   0.0        0.0\n",
       " 0.0        0.0  0.0        0.1915        0.0  0.238937   0.0        0.0\n",
       " 0.0        0.0  0.0        0.0310116  …  0.0  0.29255    0.0        0.0\n",
       " 0.0        0.0  0.0        0.180655      0.0  0.186237   0.0        0.0\n",
       " 0.0        0.0  0.0        0.128751      0.0  0.147201   0.0        0.0\n",
       " 0.0        0.0  0.0        0.0           0.0  0.20347    0.0        0.0\n",
       " 0.0        0.0  0.0        0.0           0.0  0.0        0.0        0.0\n",
       " 0.0170281  0.0  0.0        0.0        …  0.0  0.0447381  0.0015827  0.0\n",
       "\n",
       "[:, :, 31, 1] =\n",
       " 0.0         0.0          0.0        …  0.0  0.0        0.0          0.0\n",
       " 0.00388869  0.0          0.0268748     0.0  0.0462202  0.0          0.0\n",
       " 0.041912    0.0          0.0           0.0  0.0        0.0          0.0\n",
       " 0.0546206   0.0391339    0.166317      0.0  0.0        0.0          0.0\n",
       " 0.0         0.0          0.19837       0.0  0.0        0.0          0.0\n",
       " 0.081396    0.164639     0.345842   …  0.0  0.0326635  0.0637482    0.0\n",
       " 0.0         0.0          0.174186      0.0  0.0188221  0.0          0.0\n",
       " 0.0916152   0.0751026    0.0907194     0.0  0.0        0.000104101  0.0\n",
       " 0.0323499   0.00651088   0.100315      0.0  0.0        0.0          0.0\n",
       " 0.0533718   0.0          0.0876271     0.0  0.0        0.0          0.0\n",
       " 0.0758028   0.089674     0.222084   …  0.0  0.0        0.0          0.0\n",
       " 0.0387939   0.000705058  0.172426      0.0  0.0        0.0          0.0\n",
       " 0.0590758   0.0261548    0.0458518     0.0  0.0        0.0          0.0\n",
       " 0.0656786   0.0          0.0           0.0  0.0        0.0          0.0\n",
       " 0.155185    0.0704455    0.126955      0.0  0.0295589  0.0426845    0.0\n",
       " 0.194708    0.10492      0.142808   …  0.0  0.0        0.0815854    0.0\n",
       "\n",
       "[:, :, 32, 1] =\n",
       " 0.0  0.0  0.0        0.0         0.0  …  0.0         0.0  0.0  0.0475561\n",
       " 0.0  0.0  0.0        0.0         0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0         0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0         0.0     0.0         0.0  0.0  0.00231679\n",
       " 0.0  0.0  0.0        0.0         0.0     0.0         0.0  0.0  0.00268514\n",
       " 0.0  0.0  0.107321   0.0         0.0  …  0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.139558   0.0         0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.348223   0.165804    0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.251928   0.127577    0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.256921   0.0707697   0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.248734   0.0142496   0.0  …  0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.227804   0.00701426  0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.189337   0.159416    0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.034728   0.149972    0.0     0.0         0.0  0.0  0.0\n",
       " 0.0  0.0  0.0568311  0.114032    0.0     0.00606229  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0366996  0.00457771  0.0  …  0.0         0.0  0.0  0.0\n",
       "\n",
       "[:, :, 1, 2] =\n",
       " 0.0722011  0.0073238   0.0         …  0.0         0.0        0.0\n",
       " 0.212389   0.100532    0.0            0.0415855   0.0279015  0.0\n",
       " 0.0759032  0.00315342  0.0            0.0822129   0.138562   0.201713\n",
       " 0.153845   0.131934    0.0            0.0737447   0.175385   0.200141\n",
       " 0.213636   0.21954     0.0499661      0.120017    0.27982    0.243259\n",
       " 0.166791   0.208272    0.0769618   …  0.184714    0.308712   0.249123\n",
       " 0.110481   0.144854    0.129559       0.0800229   0.259672   0.219027\n",
       " 0.108186   0.147107    0.090272       0.0593128   0.304184   0.250442\n",
       " 0.125535   0.222333    0.0615297      0.0201427   0.0920423  0.223531\n",
       " 0.107616   0.219727    0.00130708     0.0104418   0.167436   0.287031\n",
       " 0.102614   0.118672    0.0         …  0.0         0.10428    0.287005\n",
       " 0.107727   0.147977    0.0716207      0.0         0.0        0.0232008\n",
       " 0.0982285  0.186299    0.148814       0.0         0.0        0.0\n",
       " 0.0919667  0.153222    0.0524915      0.0286522   0.0786312  0.137589\n",
       " 0.0        0.144649    0.00708357     0.00556514  0.203828   0.207469\n",
       " 0.0        0.0938475   0.0225283   …  0.0160111   0.061063   0.245394\n",
       "\n",
       "[:, :, 2, 2] =\n",
       " 0.0600402  0.0        0.138075    …  0.0          0.2463    0.184044\n",
       " 0.0        0.0        0.0            0.0          0.13379   0.154282\n",
       " 0.117511   0.138538   0.227294       0.0194742    0.336561  0.345396\n",
       " 0.0440407  0.0256034  0.1346         0.058133     0.342916  0.324061\n",
       " 0.0        0.0        0.0676058      0.0164284    0.286154  0.302885\n",
       " 0.0        0.0        0.104498    …  0.000476877  0.309719  0.332815\n",
       " 0.0        0.0        0.0507142      0.0259714    0.356815  0.325147\n",
       " 0.0        0.0        0.00331291     0.0          0.164833  0.265549\n",
       " 0.0        0.0        0.00493409     0.0          0.297393  0.323421\n",
       " 0.0        0.0590577  0.0923441      0.0          0.168159  0.224043\n",
       " 0.0        0.0789644  0.204718    …  0.064352     0.213526  0.232921\n",
       " 0.0        0.0        0.0864438      0.0123191    0.258946  0.266443\n",
       " 0.0        0.0        0.0            0.0          0.259558  0.291494\n",
       " 0.0        0.0        0.0533445      0.0          0.329385  0.325877\n",
       " 0.0        0.0        0.0214529      0.00681826   0.225198  0.205755\n",
       " 0.0761459  0.158554   0.104841    …  0.101658     0.298408  0.295016\n",
       "\n",
       "[:, :, 3, 2] =\n",
       " 0.0671117  0.10421   0.0        …  0.189876   0.0        0.0314425\n",
       " 0.099144   0.172035  0.0           0.0934447  0.0        0.0\n",
       " 0.321391   0.490677  0.112899      0.296215   0.0369958  0.0343661\n",
       " 0.39401    0.543477  0.0704141     0.449845   0.165165   0.0611431\n",
       " 0.315233   0.461482  0.0704294     0.421747   0.186127   0.052088\n",
       " 0.272471   0.493499  0.211105   …  0.393597   0.174505   0.0311232\n",
       " 0.262095   0.476422  0.258324      0.297473   0.206602   0.0688849\n",
       " 0.250637   0.391475  0.095986      0.165001   0.221534   0.163038\n",
       " 0.24059    0.34146   0.0349808     0.0425125  0.143607   0.210198\n",
       " 0.269403   0.431922  0.163798      0.0        0.0        0.0583144\n",
       " 0.265063   0.52615   0.29858    …  0.0133132  0.0        0.0\n",
       " 0.278281   0.550743  0.268533      0.0        0.0        0.0\n",
       " 0.278398   0.534336  0.202911      0.0        0.0        0.0\n",
       " 0.283456   0.501354  0.213031      0.159849   0.0        0.0\n",
       " 0.179374   0.385131  0.152786      0.148891   0.0396259  0.0\n",
       " 0.0        0.19331   0.108944   …  0.0978385  0.052335   0.00183032\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 2] =\n",
       " 0.0        0.0  0.0  0.0  0.0        …  0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.120224      0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0970153     0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.139155   0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0        …  0.227585   0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.279226   0.00470941  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0828365  0.0704411   0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.168431   0.156446    0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.165806   0.131046    0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0        …  0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0        0.0         0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0           0.0        0.0         0.0  0.0\n",
       " 0.0314948  0.0  0.0  0.0  0.0        …  0.0        0.0         0.0  0.0\n",
       "\n",
       "[:, :, 31, 2] =\n",
       " 0.0        0.0         0.0       …  0.0        0.0       0.0\n",
       " 0.0        0.0         0.016689     0.0515862  0.0       0.0\n",
       " 0.0        0.0         0.0          0.0575241  0.0       0.0\n",
       " 0.0        0.0         0.0          0.0132923  0.0       0.0\n",
       " 0.0529715  0.0         0.0          0.0        0.0       0.0\n",
       " 0.040578   0.0         0.0       …  0.0        0.0       0.0\n",
       " 0.0323147  0.0         0.0          0.0        0.0       0.0\n",
       " 0.0291139  0.0         0.0          0.0        0.0       0.00861498\n",
       " 0.0220495  0.0         0.0          0.0        0.0       0.0\n",
       " 0.0136246  0.0         0.0          0.0        0.0       0.0\n",
       " 0.0245037  0.0         0.0       …  0.0        0.0       0.0\n",
       " 0.0422673  0.0         0.0          0.0        0.0       0.0\n",
       " 0.0403688  0.00806081  0.0          0.0        0.0       0.0\n",
       " 0.0364649  0.0         0.0          0.0        0.0       0.0\n",
       " 0.197016   0.189968    0.274979     0.279777   0.210099  0.149652\n",
       " 0.188835   0.11544     0.147602  …  0.147321   0.166085  0.0537094\n",
       "\n",
       "[:, :, 32, 2] =\n",
       " 0.0  0.0  0.0        0.0        …  0.0          0.0  0.0  0.0595545\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0301433  0.0961637     0.0          0.0  0.0  0.0156119\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0212722\n",
       " 0.0  0.0  0.0        0.0        …  0.0          0.0  0.0  0.0371424\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0302409\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        …  0.0          0.0  0.0  0.0428694\n",
       " 0.0  0.0  0.0        0.0           0.000637096  0.0  0.0  0.0647757\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0248386\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0           0.0          0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        …  0.0          0.0  0.0  0.0\n",
       "\n",
       "[:, :, 1, 3] =\n",
       " 0.0519496  0.0365343  0.0        …  0.0        0.199397   0.0656224\n",
       " 0.183045   0.137149   0.0595949     0.0230505  0.0        0.053794\n",
       " 0.117118   0.170023   0.107245      0.0        0.0        0.12702\n",
       " 0.0997916  0.138751   0.0724668     0.0        0.0        0.0677896\n",
       " 0.0977136  0.149588   0.0604705     0.0        0.0552272  0.168202\n",
       " 0.0970855  0.200813   0.0725228  …  0.0280311  0.104265   0.170901\n",
       " 0.10564    0.195002   0.050647      0.0        0.0676663  0.163721\n",
       " 0.114335   0.179194   0.0858597     0.0        0.0300709  0.158492\n",
       " 0.121894   0.179635   0.114625      0.0701723  0.119666   0.224214\n",
       " 0.115291   0.159163   0.0770162     0.0919275  0.145974   0.233048\n",
       " 0.114363   0.150084   0.0375515  …  0.0        0.101477   0.2174\n",
       " 0.11155    0.149318   0.0358496     0.0        0.0986405  0.206124\n",
       " 0.110727   0.153513   0.0327835     0.0210305  0.112988   0.19208\n",
       " 0.116818   0.161946   0.0376309     0.0742607  0.124667   0.183638\n",
       " 0.0        0.151345   0.0189026     0.0342736  0.196994   0.215598\n",
       " 0.0        0.0993032  0.0253971  …  0.041044   0.0755491  0.245829\n",
       "\n",
       "[:, :, 2, 3] =\n",
       " 0.0        0.0         0.0872519   …  0.0        0.238     0.121251\n",
       " 0.0        0.0         0.0            0.0        0.18195   0.23258\n",
       " 0.0        0.0         0.081821       0.0        0.368075  0.461646\n",
       " 0.0        0.0         0.12736        0.0        0.360097  0.44399\n",
       " 0.0        0.0         0.160615       0.0        0.314758  0.3338\n",
       " 0.0        0.00647612  0.156159    …  0.0303376  0.350606  0.328979\n",
       " 0.0        0.0139021   0.205465       0.0        0.361859  0.314757\n",
       " 0.0        0.0         0.143475       0.0        0.312371  0.270588\n",
       " 0.0        0.0         0.0169985      0.0        0.270479  0.271451\n",
       " 0.0        0.0         0.0180801      0.0        0.354679  0.32623\n",
       " 0.0        0.0         0.0391875   …  0.0        0.316752  0.326774\n",
       " 0.0        0.0         0.0374685      0.0        0.284734  0.321406\n",
       " 0.0        0.0         0.0455794      0.0        0.33852   0.333645\n",
       " 0.0        0.0         0.047408       0.0        0.367987  0.351773\n",
       " 0.0        0.0         0.00372293     0.0        0.219762  0.209176\n",
       " 0.0754451  0.161102    0.102198    …  0.0814228  0.293721  0.299243\n",
       "\n",
       "[:, :, 3, 3] =\n",
       " 0.236874  0.371017  0.149661  …  0.137901   0.0         0.232153\n",
       " 0.21867   0.366197  0.055935     0.214434   0.0         0.190245\n",
       " 0.259637  0.512172  0.204458     0.241007   0.0         0.158837\n",
       " 0.26993   0.487169  0.181356     0.217165   0.0         0.0883348\n",
       " 0.265871  0.461269  0.129369     0.229205   0.0         0.0427318\n",
       " 0.264596  0.454841  0.141062  …  0.253593   0.00603937  0.0160585\n",
       " 0.261151  0.490828  0.2387       0.267935   0.0         0.0142186\n",
       " 0.262734  0.520459  0.297817     0.315542   0.054867    0.0493183\n",
       " 0.262416  0.518569  0.260492     0.207155   0.0         0.0247388\n",
       " 0.271309  0.501336  0.236125     0.202701   0.0         0.0276819\n",
       " 0.279138  0.50253   0.226679  …  0.223409   0.0         0.0237761\n",
       " 0.282364  0.506376  0.225373     0.213676   0.0         0.0303943\n",
       " 0.280222  0.507528  0.224067     0.229175   0.0         0.0224969\n",
       " 0.27345   0.503572  0.226305     0.21889    0.0         0.0141357\n",
       " 0.188765  0.400034  0.164074     0.143001   0.0366555   0.0\n",
       " 0.0       0.193964  0.102107  …  0.0855627  0.0471339   0.00105377\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 3] =\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0  …  0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.120676  0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.044047  0.1551     0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0602548  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0  …  0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0  …  0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0  0.0     0.0       0.0        0.0  0.0\n",
       " 0.0379965  0.0  0.0  0.0  0.0  0.0  …  0.0       0.0        0.0  0.0\n",
       "\n",
       "[:, :, 31, 3] =\n",
       " 0.0        0.0       0.0        …  0.0        0.0        0.0\n",
       " 0.0        0.0       0.0554116     0.0        0.0        0.0\n",
       " 0.0314319  0.0       0.0           0.0687643  0.010102   0.0\n",
       " 0.0352062  0.0       0.0           0.130202   0.113181   0.0\n",
       " 0.0288654  0.0       0.0           0.0478862  0.0123649  0.0\n",
       " 0.0369269  0.0       0.0        …  0.0        0.0        0.0\n",
       " 0.0380794  0.0       0.0           0.133822   0.0420225  0.0\n",
       " 0.0352623  0.0       0.0           0.0920946  0.0236719  0.0\n",
       " 0.0329208  0.0       0.0           0.0        0.0        0.0\n",
       " 0.0310101  0.0       0.0           0.0        0.0        0.0\n",
       " 0.0330098  0.0       0.0        …  0.0        0.0        0.0\n",
       " 0.0399191  0.0       0.0           0.0        0.0        0.0\n",
       " 0.0428959  0.0       0.0           0.0        0.0        0.0\n",
       " 0.0462984  0.0       0.0           0.0        0.0        0.0\n",
       " 0.190165   0.187487  0.273966      0.24691    0.204946   0.153227\n",
       " 0.192486   0.117799  0.152844   …  0.136071   0.165818   0.0478525\n",
       "\n",
       "[:, :, 32, 3] =\n",
       " 0.0  0.0  0.0  0.0  0.0950683  0.0717076  …  0.0        0.0  0.0885134\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0482593     0.110239   0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.133769      0.0365372  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0        …  0.0        0.0  0.00272384\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0554057\n",
       " 0.0  0.0  0.0  0.0  0.0        0.010199      0.0        0.0  0.0403202\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0        …  0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.00189216\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0           0.0        0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0        0.0        …  0.0        0.0  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 1, 62] =\n",
       " 0.0402145   0.0532927  0.0         …  0.0        0.0        0.0\n",
       " 0.136084    0.131067   0.0579956      0.0226797  0.0313625  0.0\n",
       " 0.00393085  0.195695   0.110324       0.0317249  0.10199    0.188508\n",
       " 0.0282376   0.215587   0.129955       0.0356544  0.111412   0.191362\n",
       " 0.0936605   0.0917379  0.104162       0.0168729  0.103795   0.196339\n",
       " 0.139101    0.0        0.0         …  0.0        0.0699561  0.176386\n",
       " 0.264688    0.107313   0.163373       0.0        0.0731262  0.179837\n",
       " 0.0691202   0.0908905  0.140297       0.0239575  0.105964   0.181819\n",
       " 0.26457     0.0867791  0.0746555      0.0        0.0881874  0.197076\n",
       " 0.355479    0.0470072  0.0            0.0318916  0.139485   0.21683\n",
       " 0.258847    0.0        0.0         …  0.0392206  0.159061   0.219992\n",
       " 0.14763     0.0450769  0.0            0.0        0.124703   0.216023\n",
       " 0.112655    0.107385   0.0374645      0.0        0.115074   0.203637\n",
       " 0.114179    0.146474   0.0412415      0.026585   0.120496   0.188031\n",
       " 0.0         0.148769   0.00716173     0.0253382  0.192482   0.215746\n",
       " 0.0         0.0938701  0.016716    …  0.0209681  0.078776   0.249195\n",
       "\n",
       "[:, :, 2, 62] =\n",
       " 0.0        0.0         0.132932   …  0.00937117  0.255927  0.192181\n",
       " 0.0        0.0         0.0           0.0         0.119302  0.155758\n",
       " 0.0        0.00294422  0.144745      0.0342986   0.363255  0.351584\n",
       " 0.0        0.0578802   0.0251003     0.0312221   0.363356  0.345785\n",
       " 0.0        0.0         0.0           0.0120975   0.364142  0.345209\n",
       " 0.0        0.0         0.0        …  0.0         0.315836  0.329891\n",
       " 0.0        0.0         0.0           0.0         0.279274  0.300337\n",
       " 0.0356554  0.0467006   0.0           0.0         0.285406  0.296111\n",
       " 0.0        0.0         0.0           0.0         0.25927   0.285484\n",
       " 0.0        0.0         0.0           0.0         0.247285  0.290694\n",
       " 0.0        0.0         0.0        …  0.0         0.310863  0.31192\n",
       " 0.0        0.0         0.0           0.0         0.317248  0.325826\n",
       " 0.0        0.0         0.0           0.0         0.30948   0.326863\n",
       " 0.0        0.0         0.0361131     0.0185753   0.346019  0.340148\n",
       " 0.0        0.0         0.0           0.0         0.224785  0.203119\n",
       " 0.077305   0.161841    0.107186   …  0.103662    0.293133  0.299875\n",
       "\n",
       "[:, :, 3, 62] =\n",
       " 0.239102  0.368458  0.129942   …  0.178955   0.0          0.0378984\n",
       " 0.26232   0.340281  0.0124        0.108987   0.0          0.0\n",
       " 0.334247  0.36326   0.0674711     0.227886   0.00737819   0.0288299\n",
       " 0.26005   0.290042  0.0363684     0.206823   0.0          0.0209539\n",
       " 0.399277  0.336486  0.0419608     0.233099   0.00115019   0.0238154\n",
       " 0.284654  0.334915  0.143536   …  0.258587   0.0196638    0.0371814\n",
       " 0.369096  0.269761  0.0722047     0.266069   0.012381     0.025468\n",
       " 0.203743  0.204486  0.241979      0.228503   0.000219628  0.0281746\n",
       " 0.318764  0.399357  0.142803      0.255263   0.0104125    0.0263095\n",
       " 0.381893  0.525558  0.312518      0.180161   0.0          0.0201484\n",
       " 0.276205  0.615226  0.351796   …  0.164997   0.0          0.0217485\n",
       " 0.287767  0.584166  0.276715      0.218019   0.0          0.0152128\n",
       " 0.280776  0.497849  0.2312        0.223509   0.0          0.0191355\n",
       " 0.283375  0.491677  0.221439      0.225146   0.0          0.0124454\n",
       " 0.195612  0.400556  0.159017      0.160647   0.0412677    0.0\n",
       " 0.0       0.203004  0.106108   …  0.0885752  0.0530901    0.00863838\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 62] =\n",
       " 0.0        0.0        0.0  0.0        …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0166817  0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.099635   0.0        0.0  0.0        …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.217565   0.146287   0.0  0.0596612     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.209327   0.0666828  0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0687285  0.0325797  0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.183765   0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0325885  0.0        0.0  0.0        …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0        0.0        0.0  0.0           0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0400271  0.0        0.0  0.0        …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 31, 62] =\n",
       " 0.0        0.0       0.0        0.0       …  0.0        0.0       0.0\n",
       " 0.0606925  0.069162  0.0957903  0.169903     0.0630472  0.0       0.0\n",
       " 0.210113   0.209317  0.305433   0.323888     0.0        0.0       0.0\n",
       " 0.166433   0.140742  0.145539   0.104264     0.0        0.0       0.0\n",
       " 0.122603   0.162815  0.269226   0.304274     0.0574294  0.0       0.0\n",
       " 0.0257226  0.0       0.171866   0.293001  …  0.117551   0.0       0.0\n",
       " 0.169144   0.159203  0.238279   0.129818     0.0666445  0.0       0.0\n",
       " 0.0211661  0.0       0.0        0.0          0.0510503  0.0       0.0\n",
       " 0.0        0.0       0.0        0.0          0.0        0.0       0.0\n",
       " 0.0        0.0       0.0        0.0          0.0        0.0       0.0\n",
       " 0.0        0.0       0.0        0.0       …  0.0        0.0       0.0\n",
       " 0.0        0.0       0.0        0.0          0.0        0.0       0.0\n",
       " 0.0240902  0.0       0.0        0.0          0.0        0.0       0.0\n",
       " 0.034876   0.0       0.0        0.0          0.0        0.0       0.0\n",
       " 0.188393   0.181821  0.275481   0.286585     0.26792    0.215526  0.155424\n",
       " 0.190857   0.113368  0.150842   0.159343  …  0.145956   0.164206  0.061296\n",
       "\n",
       "[:, :, 32, 62] =\n",
       " 0.0       0.0  0.0  0.00607718  …  0.0  0.0  0.0  0.0  0.0  0.0546462\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0       0.0  0.0  0.0465437      0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.00931284\n",
       " 0.0       0.0  0.0  0.0         …  0.0  0.0  0.0  0.0  0.0  0.0198666\n",
       " 0.138167  0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0266595\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0322153\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0222256\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.00852165\n",
       " 0.0       0.0  0.0  0.0         …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.00214013\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.00203207\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.00727747\n",
       " 0.0       0.0  0.0  0.0            0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0       0.0  0.0  0.0         …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 1, 63] =\n",
       " 0.0444384  0.0341424  0.0        …  0.0         0.0        0.0\n",
       " 0.164957   0.125924   0.036428      0.0         0.0        0.0\n",
       " 0.105932   0.141284   0.0540421     0.0         0.0        0.164005\n",
       " 0.0421981  0.0614801  0.0           0.0         0.0        0.129482\n",
       " 0.024853   0.0        0.0172659     0.109194    0.0        0.027974\n",
       " 0.110452   0.182432   0.0743802  …  0.0724476   0.0        0.11196\n",
       " 0.156913   0.212011   0.0           0.107562    0.0        0.0967085\n",
       " 0.277198   0.27171    0.0           0.0479686   0.0        0.0214707\n",
       " 0.249087   0.255982   0.0515248     0.0         0.0        0.0\n",
       " 0.170739   0.141637   0.046687      0.0         0.0        0.00594054\n",
       " 0.107946   0.132417   0.134811   …  0.0207367   0.0640917  0.114886\n",
       " 0.0945929  0.0826634  0.0801115     0.0896339   0.118517   0.198266\n",
       " 0.0913541  0.12163    0.0           0.19397     0.143644   0.174426\n",
       " 0.119399   0.303452   0.138403      0.240696    0.151122   0.189715\n",
       " 0.0        0.193076   0.109967      0.00927826  0.191021   0.228087\n",
       " 0.0        0.136674   0.245155   …  0.0         0.0530337  0.232006\n",
       "\n",
       "[:, :, 2, 63] =\n",
       " 0.0        0.0        0.00622053  …  0.12774    0.242932   0.131779\n",
       " 0.0        0.0        0.0            0.0        0.113945   0.150829\n",
       " 0.0        0.0        0.0391126      0.206251   0.189339   0.216373\n",
       " 0.0        0.0        0.045714       0.338517   0.265115   0.286336\n",
       " 0.0        0.0        0.111079       0.171259   0.138303   0.211354\n",
       " 0.0418729  0.0850439  0.138801    …  0.282806   0.235262   0.249938\n",
       " 0.148467   0.150062   0.218946       0.0850525  0.0896361  0.261661\n",
       " 0.0399564  0.0        0.0            0.0758106  0.170503   0.274771\n",
       " 0.0        0.0        0.0            0.0398928  0.292038   0.290347\n",
       " 0.0        0.063116   0.144264       0.0        0.347436   0.375679\n",
       " 0.0        0.0        0.124882    …  0.0        0.331124   0.343123\n",
       " 0.0        0.0681642  0.25324        0.0        0.354828   0.339134\n",
       " 0.0        0.16458    0.320166       0.0        0.342438   0.362224\n",
       " 0.0        0.11186    0.191414       0.0279879  0.355109   0.346653\n",
       " 0.0        0.199838   0.300246       0.11139    0.232119   0.181871\n",
       " 0.0697087  0.215896   0.162441    …  0.191603   0.308015   0.310997\n",
       "\n",
       "[:, :, 3, 63] =\n",
       " 0.239972  0.348648  0.182055   …  0.0449766  0.0        0.075168\n",
       " 0.225465  0.355202  0.079578      0.0        0.0        0.0\n",
       " 0.273429  0.505178  0.217075      0.064088   0.0        0.0\n",
       " 0.282988  0.501417  0.15055       0.154299   0.0        0.0\n",
       " 0.269782  0.427071  0.046409      0.136603   0.0        0.0\n",
       " 0.201276  0.42029   0.0629015  …  0.192194   0.0        0.0\n",
       " 0.151489  0.381295  0.115678      0.178193   0.0        0.0\n",
       " 0.104273  0.294174  0.152518      0.19236    0.0        0.0\n",
       " 0.150066  0.359351  0.109274      0.129904   0.0        0.0\n",
       " 0.218988  0.35013   0.101012      0.0948102  0.0        0.0\n",
       " 0.254917  0.447079  0.104126   …  0.136767   0.0        0.0\n",
       " 0.276995  0.438179  0.12018       0.245741   0.0        0.005871\n",
       " 0.26651   0.353654  0.122563      0.375728   0.0510523  0.0316775\n",
       " 0.283235  0.384392  0.108069      0.371158   0.0248588  0.0267082\n",
       " 0.190853  0.316435  0.168295      0.249134   0.0750226  0.0\n",
       " 0.0       0.186464  0.0856573  …  0.170142   0.0604913  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 63] =\n",
       " 0.0        0.0  0.0  0.0  0.0         …  0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0         …  0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.00350279     0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0         …  0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.143334       0.0       0.0     0.0  0.0  0.0\n",
       " 0.0        0.0  0.0  0.0  0.0            0.0       0.0     0.0  0.0  0.0\n",
       " 0.0276103  0.0  0.0  0.0  0.0         …  0.195083  0.0885  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 31, 63] =\n",
       " 0.0        0.0         0.0         …  0.0        0.0       0.0\n",
       " 0.0        0.00551815  0.0439477      0.0        0.0       0.0\n",
       " 0.0293505  0.0         0.0            0.0        0.0       0.0\n",
       " 0.0460154  0.0651104   0.1296         0.0        0.0       0.0335485\n",
       " 0.0593371  0.129257    0.143415       0.0        0.0       0.103846\n",
       " 0.142026   0.07908     0.130666    …  0.0        0.0       0.0192971\n",
       " 0.0123244  0.0         0.0            0.0        0.0       0.0102717\n",
       " 0.0        0.0         0.0            0.0        0.0       0.0\n",
       " 0.0        0.0         0.0            0.0322324  0.0       0.0151479\n",
       " 0.0128804  0.0         0.0            0.0540551  0.0       0.0\n",
       " 0.0255596  0.0         0.0         …  0.0        0.0       0.0\n",
       " 0.0490208  0.0170745   0.00313326     0.0        0.0       0.0\n",
       " 0.0422502  0.0         0.0            0.0        0.0       0.0\n",
       " 0.0176312  0.0427499   0.0690509      0.0        0.0       0.0\n",
       " 0.163304   0.22012     0.261557       0.204433   0.191963  0.163071\n",
       " 0.171052   0.0656404   0.0746828   …  0.241248   0.179406  0.0466706\n",
       "\n",
       "[:, :, 32, 63] =\n",
       " 0.0  0.0        0.0        0.0        …  0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.0860561\n",
       " 0.0  0.0329514  0.0        0.0           0.0         0.0        0.0665356\n",
       " 0.0  0.0704905  0.0        0.0        …  0.0         0.0        0.0898906\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.21853\n",
       " 0.0  0.0        0.0        0.0           0.00714721  0.114059   0.33061\n",
       " 0.0  0.0        0.0        0.0           0.0718707   0.0603562  0.258863\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.137166\n",
       " 0.0  0.0        0.0        0.0        …  0.0         0.0        0.0\n",
       " 0.0  0.0        0.0107341  0.0501639     0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.00409091\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0           0.0         0.0        0.0\n",
       " 0.0  0.0        0.0        0.0337603  …  0.0         0.0        0.0\n",
       "\n",
       "[:, :, 1, 64] =\n",
       " 0.0543404  0.049528   0.0        …  0.14017     0.0        0.0\n",
       " 0.156764   0.14963    0.0346245     0.382035    0.327717   0.0\n",
       " 0.11345    0.150025   0.0230668     0.266129    0.297823   0.179135\n",
       " 0.126902   0.148778   0.0385953     0.145358    0.276108   0.178476\n",
       " 0.119893   0.150579   0.0388916     0.0         0.103039   0.178937\n",
       " 0.114953   0.140817   0.059155   …  0.0         0.0        0.209681\n",
       " 0.0923102  0.159563   0.0631084     0.0         0.092249   0.256173\n",
       " 0.0838645  0.162114   0.0487245     0.0         0.15714    0.258431\n",
       " 0.0848932  0.0634441  0.0           0.243259    0.261069   0.37141\n",
       " 0.103445   0.140877   0.0           0.137308    0.205563   0.219178\n",
       " 0.0264103  0.181644   0.120608   …  0.0         0.0        0.152905\n",
       " 0.0        0.138187   0.152441      0.0         0.0353864  0.199837\n",
       " 0.108485   0.275121   0.376458      0.0         0.101689   0.186203\n",
       " 0.284574   0.21125    0.250198      0.0575893   0.128744   0.190057\n",
       " 0.247169   0.242721   0.131249      0.00961845  0.192275   0.213369\n",
       " 0.0        0.179949   0.0497679  …  0.00399283  0.0733407  0.251976\n",
       "\n",
       "[:, :, 2, 64] =\n",
       " 0.0        0.0        0.0         …  0.0         0.135899   0.195911\n",
       " 0.0        0.0        0.0            0.0         0.0310906  0.157813\n",
       " 0.0        0.0        0.0300262      0.0540432   0.395263   0.358006\n",
       " 0.0        0.0        0.0441256      0.00652502  0.471861   0.345573\n",
       " 0.0        0.0        0.0570207      0.111092    0.521168   0.33212\n",
       " 0.0        0.0        0.0522368   …  0.0         0.489757   0.344924\n",
       " 0.0        0.0        0.0600709      0.0         0.394799   0.334546\n",
       " 0.0        0.0        0.00829228     0.0         0.382044   0.307157\n",
       " 0.0        0.120593   0.192187       0.0         0.213105   0.233215\n",
       " 0.0        0.120455   0.270041       0.0         0.395937   0.404518\n",
       " 0.0303594  0.139231   0.295293    …  0.0         0.384636   0.387599\n",
       " 0.183815   0.42999    0.416567       0.0         0.339421   0.341243\n",
       " 0.208672   0.305752   0.329778       0.0898415   0.350049   0.342772\n",
       " 0.189685   0.124385   0.304151       0.0286822   0.368885   0.344492\n",
       " 0.0        0.0        0.0            0.0         0.235954   0.207619\n",
       " 0.0524315  0.0884994  0.0         …  0.0944868   0.287049   0.294718\n",
       "\n",
       "[:, :, 3, 64] =\n",
       " 0.252915   0.356204  0.161838   …  0.306421   0.152104   0.0359497\n",
       " 0.225886   0.357388  0.0894601     0.209517   0.0838324  0.0\n",
       " 0.265777   0.50107   0.220842      0.399676   0.390337   0.016612\n",
       " 0.278098   0.496009  0.22293       0.260293   0.339399   0.0334382\n",
       " 0.270835   0.512592  0.237764      0.228159   0.247569   0.00952835\n",
       " 0.267573   0.510148  0.239761   …  0.222233   0.146164   0.0299098\n",
       " 0.26172    0.465249  0.209039      0.374661   0.224796   0.117923\n",
       " 0.278948   0.501905  0.226021      0.355915   0.236128   0.220186\n",
       " 0.262655   0.467977  0.303739      0.28315    0.0293978  0.0937768\n",
       " 0.26197    0.437162  0.27919       0.286632   0.0        0.0161968\n",
       " 0.264457   0.310741  0.105559   …  0.126266   0.0        0.0\n",
       " 0.243571   0.171039  0.0           0.12428    0.0        0.0\n",
       " 0.244549   0.389415  0.0           0.200964   0.0        0.0189093\n",
       " 0.23316    0.437815  0.179145      0.203819   0.0        0.0165045\n",
       " 0.0815217  0.387445  0.177776      0.141375   0.0383133  0.0\n",
       " 0.0        0.217834  0.182443   …  0.0965391  0.0442329  0.00690534\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 30, 64] =\n",
       " 0.0         0.0        0.0        0.0  …  0.0        0.0        0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.0        0.0        0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.137755   0.0        0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.413296   0.255616   0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.156976   0.0968456  0.0  0.0\n",
       " 0.0         0.0        0.0        0.0  …  0.232992   0.0756932  0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.21994    0.135793   0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.0962284  0.106271   0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.081707   0.0878822  0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.0905362  0.0219927  0.0  0.0\n",
       " 0.0         0.0        0.0        0.0  …  0.0        0.0        0.0  0.0\n",
       " 0.00994374  0.0        0.0        0.0     0.0        0.0        0.0  0.0\n",
       " 0.0471668   0.0        0.0        0.0     0.0        0.0        0.0  0.0\n",
       " 0.0254644   0.0        0.0        0.0     0.0        0.0        0.0  0.0\n",
       " 0.0         0.0        0.0        0.0     0.0        0.0        0.0  0.0\n",
       " 0.0551585   0.0205244  0.0598366  0.0  …  0.0        0.0        0.0  0.0\n",
       "\n",
       "[:, :, 31, 64] =\n",
       " 0.0         0.0         0.0         …  0.0        0.0       0.0\n",
       " 0.00824793  0.00132142  0.0351212      0.0816311  0.0       0.0\n",
       " 0.0432409   0.0         0.0            0.0        0.0       0.0\n",
       " 0.029329    0.0         0.0            0.0875629  0.0       0.0\n",
       " 0.0223039   0.0         0.0            0.093257   0.0       0.0\n",
       " 0.0347017   0.0         0.0         …  0.228762   0.0       0.0\n",
       " 0.0572356   0.0         0.0            0.386456   0.126236  0.0\n",
       " 0.0366462   0.0         0.0            0.083055   0.0       0.0\n",
       " 0.0686094   0.0         0.00342822     0.0        0.0       0.0\n",
       " 0.0527228   0.0         0.0926628      0.0        0.0       0.0\n",
       " 0.176275    0.270132    0.440336    …  0.0        0.0       0.0\n",
       " 0.24127     0.219704    0.255845       0.0        0.0       0.0\n",
       " 0.277432    0.235237    0.272352       0.0        0.0       0.0\n",
       " 0.133836    0.00871797  0.0            0.0        0.0       0.0\n",
       " 0.164325    0.131834    0.289105       0.272424   0.213525  0.155134\n",
       " 0.119047    0.0608812   0.166361    …  0.152008   0.160611  0.0554892\n",
       "\n",
       "[:, :, 32, 64] =\n",
       " 0.0  0.0  0.0        0.0        0.0  …  0.0        0.0  0.0  0.0504098\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.00196292\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0626367  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.270813   0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0  …  0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0215919\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0282323\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0810979\n",
       " 0.0  0.0  0.0148674  0.132065   0.0  …  0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0587331  0.0     0.0        0.0  0.0  0.00184237\n",
       " 0.0  0.0  0.0        0.0        0.0     0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0        0.0        0.0  …  0.0        0.0  0.0  0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklayer(first(clevrDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pa, xi, f0, nd, ad) = (\"16×16×3×64 Param{KnetArray{Float32,4}}\", -0.96862745f0, 88469.3f0, 5.409804f8, -1.5159675f0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@gcheck1 newloss(checklayer, first(clevrDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "379.3747039635976\n",
      "epoch\n",
      "2\n",
      "379.3747039635976\n",
      "epoch\n",
      "3\n",
      "379.3747039635976\n",
      "epoch\n",
      "4\n",
      "379.3747039635976\n",
      "epoch\n",
      "5\n",
      "379.3747039635976\n",
      "epoch\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)",
     "output_type": "error",
     "traceback": [
      "CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)",
      "",
      "Stacktrace:",
      " [1] throw_api_error(::CUDA.cudaError_enum) at /kuacc/users/ashah20/.julia/packages/CUDA/YeS8q/lib/cudadrv/error.jl:97",
      " [2] macro expansion at /kuacc/users/ashah20/.julia/packages/CUDA/YeS8q/lib/cudadrv/error.jl:104 [inlined]",
      " [3] cuMemcpyHtoD_v2(::CUDA.CuPtr{Nothing}, ::Ptr{Float32}, ::Int64) at /kuacc/users/ashah20/.julia/packages/CUDA/YeS8q/lib/utils/call.jl:93",
      " [4] _unsafe_copy!(::KnetArray{Float32,4}, ::Int64, ::Array{Float32,4}, ::Int64, ::Int64) at /kuacc/users/ashah20/.julia/packages/Knet/C0PoK/src/knetarrays/karray.jl:98",
      " [5] convert(::Type{KnetArray{Float32,4}}, ::Array{Float64,4}) at /kuacc/users/ashah20/.julia/packages/Knet/C0PoK/src/knetarrays/karray.jl:127",
      " [6] convert(::Type{KnetArray{Float32,N} where N}, ::Array{Float64,4}) at /kuacc/users/ashah20/.julia/packages/Knet/C0PoK/src/knetarrays/karray.jl:126",
      " [7] atype(::Array{Float64,4}) at ./In[4]:3",
      " [8] iterate(::CLEVR, ::Array{Int64,1}) at ./In[4]:35",
      " [9] top-level scope at ./In[40]:13",
      " [10] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "# for p in params(SlotAttentionModel)\n",
    "#     p.opt = Adam()\n",
    "# end\n",
    "\n",
    "for epoch in 1:10\n",
    "    i = 0\n",
    "    val_loss = 0\n",
    "    for batch in clevrDataset\n",
    "#     batch = first(clevrDataset)\n",
    "    \n",
    "#         println(i)\n",
    "        adam!(loss, [(SlotAttentionModel, batch)], params=params(SlotAttentionModel), lr = 0.0004)\n",
    "        i += 1\n",
    "    end\n",
    "    if (epoch % 1 == 0)\n",
    "        println(epoch)\n",
    "        for testbatch in clevrDatasetTest\n",
    "            val_loss += loss(SlotAttentionModel, testbatch)\n",
    "        end\n",
    "        println(val_loss)\n",
    "    end\n",
    "    println(\"epoch\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "35×35 Array{RGB{N0f8},2} with eltype RGB{Normed{UInt8,8}}:\n",
       " RGB{N0f8}(0.0,0.004,0.0)     …  RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.008,0.0)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.02,0.0)         RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.016,0.0)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.0,0.035)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.0,0.047)     …  RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.008,0.0)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.016,0.0)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.02,0.0)         RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.016,0.0)        RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.004,0.016)   …  RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.004,0.016)      RGB{N0f8}(0.0,0.0,0.0)\n",
       " RGB{N0f8}(0.0,0.02,0.0)         RGB{N0f8}(0.0,0.0,0.0)\n",
       " ⋮                            ⋱  \n",
       " RGB{N0f8}(0.0,0.008,0.0)        RGB{N0f8}(0.031,0.024,0.027)\n",
       " RGB{N0f8}(0.0,0.027,0.0)        RGB{N0f8}(0.035,0.012,0.027)\n",
       " RGB{N0f8}(0.0,0.031,0.0)     …  RGB{N0f8}(0.035,0.0,0.02)\n",
       " RGB{N0f8}(0.0,0.043,0.0)        RGB{N0f8}(0.027,0.0,0.012)\n",
       " RGB{N0f8}(0.0,0.039,0.0)        RGB{N0f8}(0.024,0.0,0.008)\n",
       " RGB{N0f8}(0.0,0.016,0.0)        RGB{N0f8}(0.027,0.012,0.008)\n",
       " RGB{N0f8}(0.0,0.008,0.02)       RGB{N0f8}(0.031,0.016,0.004)\n",
       " RGB{N0f8}(0.02,0.008,0.051)  …  RGB{N0f8}(0.02,0.016,0.0)\n",
       " RGB{N0f8}(0.012,0.0,0.035)      RGB{N0f8}(0.0,0.012,0.0)\n",
       " RGB{N0f8}(0.004,0.0,0.016)      RGB{N0f8}(0.0,0.012,0.0)\n",
       " RGB{N0f8}(0.0,0.0,0.0)          RGB{N0f8}(0.0,0.016,0.0)\n",
       " RGB{N0f8}(0.0,0.0,0.0)          RGB{N0f8}(0.0,0.016,0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load(filenames[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 32, 256)\n",
      "(10, 10, 32, 256)\n",
      "(14, 14, 32, 256)\n",
      "(16, 16, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "function outputPad(x) \n",
    "    a, b, c, d = size(x)\n",
    "#     hostx = Array{Float32}(x)\n",
    "    paddedzeros = array_type(zeros(a+2, b+2, c, d))\n",
    "    paddedzeros[2:a+1, 2:b+1, :, :] .= x\n",
    "    return paddedzeros\n",
    "end\n",
    "\n",
    "struct DeConv1; w; b; f; p; pad; end\n",
    "(c::DeConv1)(x) = c.f.(deconv4(c.w, dropout(x,c.p), padding=(c.pad,c.pad), stride=1) .+ c.b)\n",
    "DeConv1(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0, pad=1) = DeConv1(param(w1,w2,cy,cx), param0(1,1,cy,1), f, pdrop, pad)\n",
    "\n",
    "deconvlayer1 = DeConv1(5,5,slot_size,hidden_dim, pad=0)\n",
    "deconvlayer2 = DeConv1(5,5,slot_size,hidden_dim, pad=0)\n",
    "deconvlayer3 = DeConv1(5,5,slot_size,hidden_dim, pad=0)\n",
    "deconvlayer4 = DeConv1(5,5,hidden_dim,4)\n",
    "\n",
    "function example(x)\n",
    "    # has dimention 2,2,32,256\n",
    "    y = deconvlayer1(x)\n",
    "    println(size(y))\n",
    "    y = deconvlayer2(y)\n",
    "    println(size(y))\n",
    "    y = deconvlayer3(y)\n",
    "    println(size(y))\n",
    "    y = deconvlayer4(y)\n",
    "    println(size(y))\n",
    "end\n",
    "\n",
    "out = example(param(2,2,32,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1mN\u001b[22m \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22minit \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22mforw \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22mparam \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22mparams \u001b[0m\u001b[1mr\u001b[22ma\u001b[0m\u001b[1mn\u001b[22md\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mr\u001b[22ma\u001b[0m\u001b[1mn\u001b[22md\u001b[0m\u001b[1mn\u001b[22m! Me\u001b[0m\u001b[1mr\u001b[22mse\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22meTwister\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{finit=ones}: Weight initialization method for the bias of forget gates.\n",
       "\n",
       "\n",
       "\\item \\texttt{atype=Knet.atype()}: array type for model weights.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `finit=ones`: Weight initialization method for the bias of forget gates.\n",
       "  * `atype=Knet.atype()`: array type for model weights.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of sequences \u001b[36mx\u001b[39m,\n",
       "  \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer for each time step.\n",
       "  \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the initial hidden states and read\n",
       "  the final hidden states of all layers. Note that the final time step of \u001b[36my\u001b[39m\n",
       "  always contains the final hidden state of the last layer, equivalent to\n",
       "  \u001b[36mrnn.h\u001b[39m for a single layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will have the\n",
       "  same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T])\n",
       "  where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is\n",
       "  for bidirectional RNNs. By default a 1-D \u001b[36mx\u001b[39m represents a single instance for\n",
       "  a single time step, a 2-D \u001b[36mx\u001b[39m represents a single minibatch for a single time\n",
       "  step, and a 3-D \u001b[36mx\u001b[39m represents a sequence of identically sized minibatches for\n",
       "  multiple time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and have\n",
       "  size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a minibatch\n",
       "  are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an array of\n",
       "  (non-increasing) integers, it gives us the batch size for each time step\n",
       "  (allowing different sequences in the minibatch to have different lengths).\n",
       "  In this case \u001b[36mx\u001b[39m will typically be 2-D with the second dimension representing\n",
       "  variable size batches for time steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m\n",
       "  should equal \u001b[36mlength(x) ÷ size(x,1)\u001b[39m. When the batch size is different in\n",
       "  every time step, hidden states will have size (H,B,L/2L) where B is always\n",
       "  the size of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields\n",
       "  (the cell state is only used by LSTM). They can be initialized during\n",
       "  construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments, or modified later by\n",
       "  direct assignment. Valid values are \u001b[36mnothing\u001b[39m (default), \u001b[36m0\u001b[39m, or an array of the\n",
       "  right type and size possibly wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the\n",
       "  initial state is assumed to be zero and the final state is discarded keeping\n",
       "  the value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be zero\n",
       "  and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is a valid\n",
       "  state, it is used as the initial state and is replaced by the final state on\n",
       "  return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will be\n",
       "  wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object is to be\n",
       "  called multiple times in a single iteration. Between iterations (i.e. after\n",
       "  diff/update) the hidden states need to be unboxed with e.g. \u001b[36mrnn.h =\n",
       "  value(rnn.h)\u001b[39m to prevent spurious dependencies. This happens automatically\n",
       "  during the backward pass for GPU RNNs but needs to be done manually for CPU\n",
       "  RNNs. See the CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and between\n",
       "        layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias vectors.\n",
       "\n",
       "    •    \u001b[36mfinit=ones\u001b[39m: Weight initialization method for the bias of forget\n",
       "        gates.\n",
       "\n",
       "    •    \u001b[36matype=Knet.atype()\u001b[39m: array type for model weights.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from the\n",
       "  recurrent input h[t-1] and the previous layer input x[t] given matrices W, R\n",
       "  and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
