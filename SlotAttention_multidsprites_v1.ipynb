{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CUDA\n",
    "using Knet\n",
    "using CUDA: CUDA, CuArray\n",
    "using Images\n",
    "using ImageMagick\n",
    "using Random\n",
    "using AutoGrad\n",
    "using Statistics\n",
    "import Base: length, size, iterate, eltype, IteratorSize, IteratorEltype, haslength, @propagate_inbounds, repeat, rand, tail\n",
    "import .Iterators: cycle, Cycle, take\n",
    "using IterTools: ncycle, takenth\n",
    "using JLD2, FileIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnetArray{Float32,N} where N"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64;\n",
    "resolution = 64; #16 for Baseline\n",
    "decoder_res = 64;\n",
    "slot_size = 32;\n",
    "num_slots = 6;\n",
    "num_iterations = 3;\n",
    "hidden_dim = 32;\n",
    "\n",
    "mlp_hidden_dim = 64;\n",
    "\n",
    "array_type=(CUDA.functional() ? KnetArray{Float32} : Array{Float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterate (generic function with 435 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atype() = array_type\n",
    "atype(x) = convert(atype(),x)\n",
    "\n",
    "function load_image(filename)\n",
    "    img = load(filename)\n",
    "    img = imresize(img, (resolution,resolution))\n",
    "    img = Float64.(channelview(img))\n",
    "    img = img[1:3,:,:]\n",
    "    img = permutedims(img, [2, 3, 1])\n",
    "    img = img .* 2 .- 1\n",
    "end\n",
    "\n",
    "struct CLEVR\n",
    "    images\n",
    "    batchsize::Int\n",
    "    num_instances::Int\n",
    "    shuffle::Bool\n",
    "    function CLEVR(datasetPaths; batchsize::Int=32, shuffle::Bool=false)\n",
    "        nFullBatches, rem = divrem(size(datasetPaths)[end], batchsize)\n",
    "        new(datasetPaths[1:nFullBatches*batchsize], batchsize, nFullBatches*batchsize, shuffle)\n",
    "    end\n",
    "end\n",
    "\n",
    "function length(d::CLEVR)\n",
    "    nFullBatches, rem = divrem(d.num_instances, d.batchsize)\n",
    "    nFullBatches + (rem > 0)*1\n",
    "end\n",
    "\n",
    "function iterate(d::CLEVR, state=ifelse(d.shuffle, randperm(d.num_instances), collect(1:d.num_instances)))\n",
    "    if length(state) > 0\n",
    "        imgBatch = load_image.(d.images[state[1:(length(state) < d.batchsize ? end : d.batchsize)]])\n",
    "        batch = cat(imgBatch..., dims = 4)\n",
    "        state  = state[d.batchsize+1:end]\n",
    "        return atype(batch), state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"//kuacc//users//ashah20//datasets//multi_dsprites//colored_on_grayscale//train//images\"\n",
    "files = readdir(trainDataPath);\n",
    "filenames = files[endswith.(files, \".jpg\")]\n",
    "filenames = [\"$(trainDataPath)//$(file)\" for file in filenames];\n",
    "\n",
    "clevrDataset = CLEVR(filenames, batchsize=bs, shuffle = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPath = \"//kuacc//users//ashah20//datasets//multi_dsprites//colored_on_grayscale//val//images\"\n",
    "tstfiles = readdir(testDataPath);\n",
    "tstfilenames = tstfiles[endswith.(tstfiles, \".jpg\")]\n",
    "tstfilenames = [\"$(testDataPath)//$(file)\" for file in tstfilenames];\n",
    "\n",
    "clevrDatasetTest = CLEVR(tstfilenames, batchsize=bs, shuffle = false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the SlotAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Dense\n",
    "    w\n",
    "    b\n",
    "    f\n",
    "    p\n",
    "    bias\n",
    "end\n",
    "\n",
    "function Dense(i::Int,o::Int;f=relu,pdrop=0, bias=true)\n",
    "    w = param(o,i, atype = array_type)\n",
    "    if(bias)\n",
    "        b = param0(o, atype = array_type)\n",
    "    else\n",
    "        b = 0\n",
    "    end\n",
    "    Dense(w, b, f, pdrop, bias)\n",
    "end\n",
    "\n",
    "function (d::Dense)(x)\n",
    "    if(d.bias)\n",
    "        d.f.(d.w * mat(dropout(x,d.p)) .+ d.b)\n",
    "    else\n",
    "        d.f.(d.w * mat(dropout(x,d.p)))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv; w; b; pad; f; p; end\n",
    "(c::Conv)(x) = c.f.(conv4(c.w, dropout(x,c.p), padding=(c.pad,c.pad), stride=1) .+ c.b)\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,pad::Int; f=relu, pdrop=0) = Conv(param(w1,w2,cx,cy, atype = array_type), param0(1,1,cy,1, atype = array_type), pad, f, pdrop)\n",
    "\n",
    "struct DeConv; w; b; f; p; pad; stride; end\n",
    "(c::DeConv)(x) = c.f.(deconv4(c.w, dropout(x,c.p), padding=(c.pad,c.pad), stride=c.stride) .+ c.b)\n",
    "DeConv(w1::Int,w2::Int,cx::Int,cy::Int;f=relu,pdrop=0, pad=1, stride=1) = DeConv(param(w1,w2,cy,cx, atype = array_type), param0(1,1,cy,1, atype = array_type), f, pdrop, pad, stride)\n",
    "\n",
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make3d_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function flatten(x)\n",
    "    batchsize = size(x)[end]\n",
    "    return array_type(permutedims(reshape(x,(size(x)[1]*size(x)[2],hidden_dim,batchsize)),(2,1,3)))\n",
    "end\n",
    "\n",
    "function printdims(x)\n",
    "    println(size(x))\n",
    "   return x\n",
    "end\n",
    "\n",
    "function make3d_2d(x)\n",
    "   return (reshape(x,(size(x)[1],size(x)[2]*size(x)[3])))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "mutable struct LayerNorm; a; b; ϵ; dim; end \n",
    "\n",
    "\"\"\"\n",
    "    LayerNorm(dmodel)\n",
    "Creates an layer normalization layer. Inputs should be hidden vectors with hidden size of dmodel\n",
    "Input shape: Tensor of arbitrary number of hidden vectors [dmodel, o...]\n",
    "Output shape: Identical shape of [dmodel, o...]\n",
    "\"\"\"\n",
    "function LayerNorm(dmodel; dim=1, eps=1e-12, atype=atype())\n",
    "    a = param(dmodel; init=ones, atype=atype)\n",
    "    b = param(dmodel; init=zeros, atype=atype)\n",
    "    LayerNorm(a, b, eps, dim)\n",
    "end\n",
    "\n",
    "\n",
    "function (l::LayerNorm)(x, o...)\n",
    "    μ = mean(x,dims=l.dim)\n",
    "    # Albert Implementation uses corrected == false when testing\n",
    "    # Source: https://github.com/huggingface/transformers/blob/b592728eff9996e2cff1c5107438c4989aaa8149/src/transformers/models/albert/modeling_albert.py#L239 \n",
    "    σ = std(x,mean=μ,dims=l.dim, corrected=false)\n",
    "    ϵ = eltype(x)(l.ϵ)\n",
    "    l.a .* (x .- μ) ./ (σ .+ ϵ) .+ l.b # TODO: doing x .- μ twice?\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slot Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now the slot attention module will get this encoded features matrix####\n",
    "### input dimensions of this feature matrix is channel, hxw, batchsize\n",
    "\n",
    "mutable struct SlotAttention\n",
    "    num_iterations\n",
    "    num_slots\n",
    "    slot_size\n",
    "    project_k\n",
    "    project_v\n",
    "    project_q\n",
    "    gru\n",
    "    mu\n",
    "    log_sigma\n",
    "    slot_mlp\n",
    "    input_norm_layer\n",
    "    slot_norm_layer\n",
    "    mlp_norm_layer\n",
    "end\n",
    "\n",
    "\n",
    "function SlotAttention(num_iterations, num_slots, slot_size, mlp_hidden_dim)\n",
    "    project_k = Dense(slot_size, slot_size, bias=false)\n",
    "    project_v = Dense(slot_size, slot_size, bias=false)\n",
    "    project_q = Dense(slot_size, slot_size, bias=false)\n",
    "\n",
    "    gru = RNN(slot_size, slot_size, rnnType=:gru, atype = array_type)\n",
    "    \n",
    "    mu = param(slot_size, 1, 1, atype=array_type, init=xavier_uniform)\n",
    "    log_sigma = param(slot_size, 1, 1, atype=array_type, init=xavier_uniform)\n",
    "    \n",
    "    slot_mlp = Chain(Dense(slot_size, mlp_hidden_dim),\n",
    "                    Dense(mlp_hidden_dim, slot_size, f=identity))\n",
    "    \n",
    "    input_norm_layer = LayerNorm(hidden_dim, atype=array_type)\n",
    "    slot_norm_layer = LayerNorm(slot_size, atype=array_type)\n",
    "    mlp_norm_layer = LayerNorm(slot_size, atype=array_type)\n",
    "    \n",
    "    return SlotAttention(num_iterations, num_slots, slot_size, project_k, project_q, project_v, gru, mu, log_sigma, slot_mlp, input_norm_layer, slot_norm_layer, mlp_norm_layer)\n",
    "end\n",
    "\n",
    "\n",
    "attn_norm_factor = slot_size ^ -0.5\n",
    "epsilon=1e-8\n",
    "\n",
    "\n",
    "function (s::SlotAttention)(x)\n",
    "    batchsize = size(x)[end]\n",
    "#     println(size(x))\n",
    "    encoded = permutedims(reshape(x,(size(x)[1]*size(x)[2],hidden_dim,batchsize)),(2,1,3))\n",
    "    encoded = s.input_norm_layer(encoded)\n",
    "    \n",
    "    flattened_enc = make3d_2d(encoded)\n",
    "#     println(size(flattened_enc))\n",
    "\n",
    "    k = s.project_k(flattened_enc)\n",
    "    k = reshape(k, size(encoded)) #shape = [slot_size, hxw, bs]\n",
    "\n",
    "    v = s.project_v(flattened_enc)\n",
    "    v = reshape(v, size(encoded)) #shape = [slot_size, hxw, bs]\n",
    "    \n",
    "    slots = s.mu .+ exp.(s.log_sigma) .* array_type(randn(s.slot_size, s.num_slots, batchsize))\n",
    "\n",
    "\n",
    "    for i in 1:s.num_iterations\n",
    "        prev_slots = slots\n",
    "        prev_slots = make3d_2d(prev_slots)\n",
    "        s.gru.h = prev_slots\n",
    "        \n",
    "        slots = s.slot_norm_layer(slots)\n",
    "        slots = make3d_2d(slots)\n",
    "        \n",
    "        q = s.project_q(slots)\n",
    "        q = reshape(q, (s.slot_size, s.num_slots, batchsize)) #shape = [slot_size, num_slots, bs]\n",
    "\n",
    "        # batch matrix multiplication\n",
    "        attn_logits = bmm(permutedims(q,(2, 1, 3)),(attn_norm_factor .* k))\n",
    "        # softmax function\n",
    "#         attn = exp.(attn_logits) ./ sum(exp.(attn_logits), dims=1) \n",
    "        attn = softmax(attn_logits, dims=1)\n",
    "\n",
    "#         attn = attn .+ epsilon\n",
    "        attn = attn ./ (sum(attn, dims=2) .+ epsilon)#shape = [num_slots, hxw, bs]\n",
    "\n",
    "        updates = bmm(v, permutedims(attn,(2, 1, 3))) #shape = [slot_size, num_slots, bs]\n",
    "        updates = make3d_2d(updates)\n",
    "        \n",
    "        slots = s.gru(updates) #shape = [slot_size, num_slots*bs]\n",
    "#         slots = prev_slots\n",
    "        slots = slots + s.slot_mlp(s.mlp_norm_layer(slots))\n",
    "        slots = reshape(slots, (s.slot_size, s.num_slots, batchsize)) #shape = [slot_size, num_slots, bs]\n",
    "\n",
    "    end\n",
    "    \n",
    "    return slots\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function decoder_in_reshape(x)\n",
    "    batchsize = size(x)[end]\n",
    "    a = reshape(x, (1,1,slot_size, num_slots*batchsize))\n",
    "    b = array_type(ones(decoder_res, decoder_res, slot_size, num_slots*batchsize))\n",
    "    z = b .* a\n",
    "   return z\n",
    "end\n",
    "\n",
    "function postprocess(decoder_out)\n",
    "    decoder_out = reshape(decoder_out, (resolution, resolution, 4, num_slots, bs))\n",
    "\n",
    "    recons = decoder_out[:,:,1:3,:,:]\n",
    "    masks = decoder_out[:,:,4:4,:,:]\n",
    "    masks = softmax(masks, dims=4)\n",
    "\n",
    "    final_recons = reshape(sum(recons .* masks, dims=4), (resolution, resolution, 3, bs))\n",
    "    return final_recons, recons, masks\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dim(x::Array) = reshape(x, (size(x)...,1))\n",
    "\n",
    "function build_grid(resolution)\n",
    "    linspace1 = LinRange(0.0, 1.0, resolution[1])\n",
    "    linspace2 = LinRange(0.0, 1.0, resolution[2])\n",
    "    grid1 = (linspace1' .* ones(resolution[2]))'\n",
    "    grid2 = (ones(resolution[1])' .* linspace2)'\n",
    "    grid = cat(grid1,grid2; dims=3)\n",
    "    grid = permutedims(grid, [3, 1, 2])\n",
    "    grid = add_dim(grid)\n",
    "    grid = cat(grid, 1 .- grid; dims=1)\n",
    "    return grid\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct SoftPositionEmbed\n",
    "    dense\n",
    "    grid\n",
    "end\n",
    "\n",
    "function SoftPositionEmbed(hidden_size::Int, resolution)\n",
    "    dense  = Dense(4, hidden_size)\n",
    "    grid = array_type(build_grid(resolution))\n",
    "    return SoftPositionEmbed(dense, grid)\n",
    "end\n",
    "\n",
    "function (s::SoftPositionEmbed)(x)\n",
    "    orig_grid_shape = size(s.grid)\n",
    "    s.grid = reshape(s.grid,(size(s.grid)[1],size(s.grid)[2]*size(s.grid)[3]*size(s.grid)[4]))\n",
    "    emb_proj = s.dense(s.grid)\n",
    "    s.grid = reshape(s.grid,orig_grid_shape)\n",
    "    emb_proj = reshape(emb_proj,(size(emb_proj)[1],orig_grid_shape[2],orig_grid_shape[3],orig_grid_shape[4]))\n",
    "    emb_proj = permutedims(emb_proj, [2, 3, 1, 4])\n",
    "    return x .+ emb_proj\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234);\n",
    "\n",
    "res = (resolution, resolution)\n",
    "hidden_size = slot_size\n",
    "encoder_pos_embed = SoftPositionEmbed(hidden_size, res)\n",
    "\n",
    "encoder = Chain(Conv(5,5,3,hidden_dim,2),\n",
    "                Conv(5,5,hidden_dim,hidden_dim,2),\n",
    "                Conv(5,5,hidden_dim,hidden_dim,2),\n",
    "                Conv(5,5,hidden_dim,hidden_dim,2),\n",
    "                encoder_pos_embed,\n",
    "                Conv(1,1,hidden_dim,hidden_dim,0),\n",
    "                Conv(1,1,hidden_dim,slot_size,0, f=identity))\n",
    "\n",
    "\n",
    "\n",
    "decoder =  Chain(DeConv(5,5,slot_size,hidden_dim, pad=2, stride=1),\n",
    "                DeConv(5,5,hidden_dim,hidden_dim, pad=2, stride=1),\n",
    "                DeConv(5,5,hidden_dim,hidden_dim, pad=2, stride=1),\n",
    "                DeConv(3,3,hidden_dim,4, f=identity, stride=1))\n",
    "\n",
    "dec_res = (decoder_res, decoder_res)\n",
    "decoder_pos_embed = SoftPositionEmbed(hidden_size, dec_res)\n",
    "\n",
    "SlotAttentionModel = Chain(encoder,\n",
    "                        SlotAttention(num_iterations, num_slots, slot_size, mlp_hidden_dim),\n",
    "                        decoder_in_reshape,\n",
    "                        decoder_pos_embed,\n",
    "                        decoder,\n",
    "                        postprocess);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Array{Param,1}:\n",
       " P(KnetArray{Float32,4}(5,5,3,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,2}(32,4))\n",
       " P(KnetArray{Float32,1}(32))\n",
       " P(KnetArray{Float32,4}(1,1,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(1,1,32,32))\n",
       " ⋮\n",
       " P(KnetArray{Float32,1}(32))\n",
       " P(KnetArray{Float32,1}(32))\n",
       " P(KnetArray{Float32,2}(32,4))\n",
       " P(KnetArray{Float32,1}(32))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(5,5,32,32))\n",
       " P(KnetArray{Float32,4}(1,1,32,1))\n",
       " P(KnetArray{Float32,4}(3,3,4,32))\n",
       " P(KnetArray{Float32,4}(1,1,4,1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(SlotAttentionModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mse(predictions, targets)\n",
    "    N = length(targets)\n",
    "    y = 1/(2*N) * sum((predictions .- targets).^2)\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25478585561116535"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = first(clevrDataset)\n",
    "\n",
    "final_recons, recons, masks = SlotAttentionModel(x);\n",
    "error = mse(final_recons, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, input_batch)\n",
    "    final_recons, recons, masks = model(input_batch)\n",
    "    loss = mse(final_recons, input_batch)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro gcheck1(ex); esc(:(@gcheck $ex (delta=0.000001, nsample=2, rtol=0.05, atol=0.001, verbose=1))); end\n",
    "Random.seed!(123);\n",
    "sample = first(clevrDataset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkloss (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklayer = Conv(5,5,3,16,2)\n",
    "\n",
    "function checkloss(model, input_batch)\n",
    "    recon = model(input_batch)\n",
    "    loss = mean(recon)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K32(5,5,3,32)[-0.0008865529⋯]\n"
     ]
    }
   ],
   "source": [
    "L = @diff loss(SlotAttentionModel, sample)\n",
    "p = params(SlotAttentionModel)[1]\n",
    "g = grad(L, p)\n",
    "println(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Symbol,1}:\n",
       " :SlotAttentionModel"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load \"//kuacc//users//ashah20//slotattentionmodels//multi_dsprites//slotattention_v534_30.jld2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "0.1055417001247406\n",
      "32\n",
      "0.10155657132466633\n",
      "33\n",
      "0.09998173515001932\n",
      "34\n",
      "0.09271242121855418\n",
      "35\n",
      "0.0889439155658086\n",
      "36\n",
      "0.08887444535891215\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "#training hyperparameters\n",
    "num_epochs = 534\n",
    "base_learning_rate = 0.0004\n",
    "total_num_steps = length(clevrDataset) * num_epochs\n",
    "warmup_steps = 0.02 * total_num_steps\n",
    "decay_rate = 0.5\n",
    "decay_steps = 0.2 * total_num_steps\n",
    "\n",
    "# temporary variables\n",
    "current_step = 30 * length(clevrDataset)\n",
    "val_losses = zeros(num_epochs)\n",
    "    \n",
    "for batch in ncycle(clevrDataset, num_epochs)\n",
    "    current_step+=1\n",
    "    epoch = current_step/length(clevrDataset)\n",
    "   \n",
    "    \n",
    "    if (current_step % length(clevrDataset) == 0)\n",
    "        println(Int(epoch))\n",
    "        val_loss=0\n",
    "        for testbatch in clevrDatasetTest\n",
    "            val_loss += loss(SlotAttentionModel, testbatch)\n",
    "        end\n",
    "        val_loss = val_loss ./ length(clevrDatasetTest)\n",
    "        println(val_loss)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    if current_step < warmup_steps\n",
    "        learning_rate = base_learning_rate*(current_step/warmup_steps)\n",
    "    else\n",
    "        learning_rate = base_learning_rate*(decay_rate^(current_step/decay_steps))\n",
    "    end\n",
    "    \n",
    "    adam!(loss, [(SlotAttentionModel, batch)], params=params(SlotAttentionModel), lr = learning_rate)\n",
    "    \n",
    "    if (epoch % 30 == 0)\n",
    "        save_path = string(\"//kuacc//users//ashah20//slotattentionmodels//multi_dsprites//slotattention_v534_\", Int(epoch), \".jld2\")\n",
    "        @save save_path SlotAttentionModel\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save \"slotattention_v534_multi_dsprites.jld2\" SlotAttentionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @load \"slotattention_v0_tetrominoes.jld2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "input_samples = first(clevrDatasetTest);\n",
    "\n",
    "function displaySamples(input; num_samples = 8)\n",
    "    in_sample = input\n",
    "    in_sample = in_sample .* 0.5 .+ 0.5\n",
    "    in_sample = in_sample .* (in_sample.>0)\n",
    "    in_sample = in_sample .* (in_sample.<1)\n",
    "    in_sample = Array{Float32}(in_sample);\n",
    "\n",
    "    out_samples, slots_samples, masks_samples = SlotAttentionModel(input)\n",
    "    out_samples = out_samples .* 0.5 .+ 0.5\n",
    "    out_samples = out_samples .* (out_samples.>0)\n",
    "    out_samples = out_samples .* (out_samples.<1)\n",
    "    out_samples = Array{Float32}(out_samples);\n",
    "\n",
    "    slots_recons = slots_samples.*masks_samples\n",
    "    slots_recons = slots_recons .* 0.5 .+ 0.5\n",
    "    slots_recons = slots_recons .* (slots_recons.>0)\n",
    "    slots_recons = slots_recons .* (slots_recons.<1)\n",
    "    slots_recons = Array{Float32}(slots_recons);\n",
    "    \n",
    "    image_row = cat(in_sample[:,:,:,1], out_samples[:,:,:,1], dims=2)\n",
    "    for i in 1:num_slots\n",
    "        image_row = cat(image_row, slots_recons[:,:,:,i,1], dims=2)\n",
    "    end\n",
    "    image_grid = image_row\n",
    "    \n",
    "    for i in 2:num_samples\n",
    "        image_row = cat(in_sample[:,:,:,i], out_samples[:,:,:,i], dims=2)\n",
    "        for j in 1:num_slots\n",
    "            image_row = cat(image_row, slots_recons[:,:,:,j,i], dims=2)\n",
    "        end\n",
    "        image_grid = cat(image_grid, image_row, dims=1)\n",
    "    end\n",
    "    imshow(image_grid)\n",
    "end\n",
    "\n",
    "displaySamples(input_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
